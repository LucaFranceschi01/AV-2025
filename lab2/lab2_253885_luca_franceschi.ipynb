{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iqHT_h8gAQm0"
   },
   "source": [
    "# Lab 7: Lead generation and churn\n",
    "\n",
    "# Lab goals:\n",
    "In this project, we will develop and apply different methods for lead generation and churn prediction. These methodologies are broadly used in business to multiple use cases as:\n",
    "- Identify **new customers** in the market\n",
    "- Identify customers in our internal Data Warehouse with **more likely** to buy a new product\n",
    "- Identify unsatisfied customers and thus, likely to be **churners**\n",
    "\n",
    "During this project we will follow the end-to-end **Machine Learning process** to identify new customers to capture and improve sales in a **marketing use case**:\n",
    "1. Data understanding and preparation: exploration of the dataset and feature engineering (missing values, outlier identification, categorical variables management)\n",
    "2. Model Training: training the Logistic Regression Model. Analysis of metrics (recall, precision, confusion metrics)\n",
    "3. Creating a Business opportunity with Machine Learning: selection of the best model and identification of the most important features\n",
    "\n",
    "# Practice Information:\n",
    "**Due date:** during the lab session (6.30 -9pm)\n",
    "\n",
    "**Submission procedure:** via Moodle.\n",
    "\n",
    "**Name:** Luca Franceschi\n",
    "\n",
    "**NIA:** 253885\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ibGsc62Qxi_4"
   },
   "source": [
    "# 0. Context:  \n",
    "\n",
    "Let's assume that we are a Data Scientist team of a bank, and we want to predict which customers are going to leave us (**churn**) in order to be able to anticipate and take commercial action on them. To do this, the bank has been collecting tabular data on its customers including which customers have left us in the past, so we will use the power of **machine learning** to build a predictive model.\n",
    "\n",
    "### Dataset:\n",
    "\n",
    "Taking a closer look, we see that the dataset contains 14 columns (also known as features or variables). The first 13 columns are the independent variable, while the last column is the dependent variable that contains a binary value of 1 or 0. Here, 1 refers to the case where the customer left the bank after 6 months, and 0 is the case where the customer didn't leave the bank after 6 months.\n",
    "\n",
    "It's important to mention that the data for the independent variables was collected 6 months before the data for the dependent variable, since the task is to develop a machine learning model that can predict whether a customer will leave the bank after 6 months, depending on the current feature values.\n",
    "\n",
    "Let's discuss each column one by one:\n",
    "\n",
    "1. **RowNumber** corresponds to the record (row) number and has no effect on the output. This column will be removed.\n",
    "2. **CustomerId** contains random values and has no effect on customer leaving the bank. This column will be removed.\n",
    "3. **Surname** the surname of a customer has no impact on their decision to leave the bank. This column will be removed.\n",
    "4. **CreditScore** can have an effect on customer churn, since a customer with a higher credit score is less likely to leave the bank.\n",
    "5. **Geography** a customer's location can affect their decision to leave the bank. We'll keep this column.\n",
    "6. **Gender** it's interesting to explore whether gender plays a role in a customer leaving the bank. We'll include this column, too.\n",
    "7. **Age** this is certainly relevant, since older customers are less likely to leave their bank than younger ones.\n",
    "8. **Tenure** refers to the number of years that the customer has been a client of the bank. Normally, older clients are more loyal and less likely to leave a bank.\n",
    "9. **Balance** also a very good indicator of customer churn, as people with a higher balance in their accounts are less likely to leave the bank compared to those with lower balances.\n",
    "10. **NumOfProducts** refers to the number of products that a customer has purchased through the bank.\n",
    "11. **HasCrCard** denotes whether or not a customer has a credit card. This column is also relevant, since people with a credit card are less likely to leave the bank.\n",
    "12. **IsActiveMember** active customers are less likely to leave the bank, so we'll keep this.\n",
    "13. **EstimatedSalary** as with balance, people with lower salaries are more likely to leave the bank compared to those with higher salaries.\n",
    "14. **Exited** whether or not the customer left the bank. This is what we have to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7_N6iLH6AQm2"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import arviz as az\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,roc_auc_score, roc_curve\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "# Set Pandas to show all the columns\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ps7lkUkQJmuk"
   },
   "source": [
    "# **1. Data understanding and preparation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "25Sj9ucPJmuk"
   },
   "source": [
    "Before diving into predictive models, it's crucial to have a good understanding of the data at hand. This phase will walk you through loading, exploring, and preparing the dataset for further analysis.\n",
    "\n",
    "All this process is known as **Data Wrangling**. In particular, the whole data wrangling process implies:\n",
    "- Define and apply an strategy for nulls and coding for categorical variables\n",
    "- Analyze the variables distribution and correlation between them\n",
    "- Remove outliers\n",
    "- etc....\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U6v0AQhYAQnD"
   },
   "source": [
    "### **1.1 Read the data**\n",
    "\n",
    "Let's open the csv with separator \",\" and assign to a dataframe variable (use read_csv from Pandas library). Let's see the top 5 elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WA1GOkypAQnE"
   },
   "outputs": [],
   "source": [
    "# Define the parameters to read the data\n",
    "root_path = \"data/\"\n",
    "dataset = \"Churn_Modelling.csv\"\n",
    "sep = \",\"\n",
    "encoding = \"utf-8\"\n",
    "\n",
    "# Read the data\n",
    "original_data = pd.read_csv(filepath_or_buffer=os.path.abspath(os.path.join(root_path, dataset)), sep = sep, encoding = encoding)\n",
    "\n",
    "data = original_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eum1U_5xAQnH"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vppBo4GMAQnJ"
   },
   "source": [
    "\n",
    "### **1.2 Dataset Exploratory Data Analysis (EDA)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yZdXjlz0Jmup"
   },
   "source": [
    "[**EX1**] Let's identify the type of the variables (integer, float, chart...) and the size of the dataset and the file. Which are the variable with more nulls? And with no nulls?\n",
    "\n",
    "**Question**\n",
    "\n",
    "- How many records does the data have?\n",
    "\n",
    "- How many nulls does each attribute have?\n",
    "\n",
    "- Has each attribute the right datatype?\n",
    "\n",
    "Tip: [.info()](https://www.geeksforgeeks.org/python-pandas-dataframe-info/) is a function that reports the main characteristics of a dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7sdpHf9NAQnK"
   },
   "source": [
    "**Solution**\n",
    "\n",
    "Out of 10000 records, there is not a single NULL. All datatypes seem to be correct (object could be string but should be correct)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bQ_sxj0tAQnL"
   },
   "outputs": [],
   "source": [
    "# CODE HERE\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ac3WaGyAJmup"
   },
   "source": [
    "[**EX2**] Explore the dataset using descriptive statistics to understand the distribution of numerical and categorical variables.\n",
    "\n",
    "Tip: Use .describe() for descriptive statistics and seaborn or matplotlib for visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "edfg0-asAQnO"
   },
   "source": [
    "**Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6r7mXRCVAQnP"
   },
   "outputs": [],
   "source": [
    "# CODE HERE\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data=data[['CreditScore', 'Gender', 'Age', 'Tenure', 'Balance', 'EstimatedSalary', 'Exited']], hue='Exited', markers=[\"o\", \"s\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yxDTcecIAQne"
   },
   "source": [
    "**Check the relationship along Churn attribute and numerical ones**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pKZCLH7PD9Z4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gFvNGYeTAQne"
   },
   "outputs": [],
   "source": [
    "# Define the target column\n",
    "target_variable = [\"Exited\"]\n",
    "\n",
    "# CODE HERE\n",
    "\n",
    "# Define the numerical variables # Remove 'RowNumber', 'CustomerId', 'Surname', 'Exited'\n",
    "num_variables = ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'EstimatedSalary']\n",
    "\n",
    "# Define the categorical ones # Remove 'RowNumber', 'CustomerId', 'Surname', 'Exited'\n",
    "categorical_variables = ['Geography', 'Gender', 'HasCrCard', 'IsActiveMember']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I97CFo3KJmu3"
   },
   "source": [
    "#### Draw a box plot for numerical variables and distribution plot against each churn value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "34T-XFkgaDSH",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Set the aesthetic style of the plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Determine the number of rows and columns for the subplot grid\n",
    "num_vars = len(num_variables)\n",
    "cols = 2  # Two plots for each variable: boxplot and distribution plot\n",
    "rows = num_vars  # One row for each variable\n",
    "\n",
    "# Create a figure\n",
    "fig, axs = plt.subplots(rows, cols, figsize=(15, 5*rows))\n",
    "\n",
    "# Box plots and distribution plots for numerical variables\n",
    "for i, column in enumerate(num_variables):\n",
    "    sns.boxplot(x='Exited', y=column, data=data, ax=axs[i, 0])\n",
    "    axs[i, 0].set_title(f'Box plot of {column}')\n",
    "\n",
    "    sns.histplot(data=data, x=column, hue='Exited', kde=True, element='step', ax=axs[i, 1])\n",
    "    axs[i, 1].set_title(f'Distribution of {column} against churn value')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lbZQsxuWAQnm"
   },
   "source": [
    "**Check the relationship along Churn attribute and categorical ones**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ju5NNfXlAQnn"
   },
   "source": [
    "[**EX3**] Draw a box plot for categorical variables and distribution plot against each churn value.\n",
    "\n",
    "Tip: Use the same structure used for numerical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ltBnBtySAQnn"
   },
   "source": [
    "**Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kEtW8GhBaDSH"
   },
   "outputs": [],
   "source": [
    "# CODE HERE (Tip: Similar to numeric features)\n",
    "\n",
    "# Set the aesthetic style of the plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Determine the number of rows and columns for the subplot grid\n",
    "num_vars = len(categorical_variables)\n",
    "cols = 2  # Two plots for each variable: boxplot and distribution plot\n",
    "rows = num_vars  # One row for each variable\n",
    "\n",
    "# Create a figure\n",
    "fig, axs = plt.subplots(rows, cols, figsize=(15, 5*rows))\n",
    "\n",
    "# Box plots and distribution plots for numerical variables\n",
    "for i, column in enumerate(categorical_variables):\n",
    "    sns.boxplot(x='Exited', y=column, data=data, ax=axs[i, 0])\n",
    "    axs[i, 0].set_title(f'Box plot of {column}')\n",
    "\n",
    "    sns.histplot(data=data, x=column, hue='Exited', kde=True, element='step', ax=axs[i, 1])\n",
    "    axs[i, 1].set_title(f'Distribution of {column} against churn value')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1J6WXxoA1aIR"
   },
   "source": [
    "[**EX4**]\n",
    "\n",
    "List 2 insights that you can extract from the plots of the numerical and categorical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ibkDoxON1aIR"
   },
   "source": [
    "**Solution**\n",
    "\n",
    "Higher salary estimations seem to correlate to not exiting. Also, box plots are not useful with categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VAb6Fs1WAQnq"
   },
   "outputs": [],
   "source": [
    "sns.pairplot(data[num_variables + target_variable], hue=\"Exited\", height=5.5,diag_kind=\"kde\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WUFTu5APaDSH"
   },
   "source": [
    "#### Correlation matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HHfr9ZzSaDSH"
   },
   "outputs": [],
   "source": [
    "corr_matrix = data.corr(numeric_only=True)\n",
    "sns.set (rc = {'figure.figsize':(15, 10)})\n",
    "sns.heatmap(corr_matrix, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ypVA9uwBAQns"
   },
   "source": [
    "[**EX5**]\n",
    "\n",
    "Do you appreciate any variable that is highly related with the churn value? Which and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0lJkSfwBAQns"
   },
   "source": [
    "**Solution**\n",
    "\n",
    "Number of products and Age seem to be quite correlated with churn value. The former negatively and the latter positively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LlEIUCxxAQnt"
   },
   "source": [
    "### **1.3 Data wrangling**\n",
    "\n",
    "Once the dataset has been explored, the next step is to clean it. This process, known as Data Wrangling, is composed by processes to remove null values, standardize them, removing fields that are not of interest for our application, and so on.\n",
    "\n",
    "In this case we do not detect null vales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Otf5uIBJaDSI"
   },
   "source": [
    "#### Encode binary cathegorical data as Gender:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SR-aCYyOaDSI"
   },
   "outputs": [],
   "source": [
    "data['Gender'] = np.where(data['Gender']=='Female', 1, 0)\n",
    "data['Gender'].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WkEelI3BAQn0"
   },
   "source": [
    "[**EX 6**]\n",
    "\n",
    "Encode categorical variables (those that are not binary) with one-hot-encoding\n",
    "\n",
    "Tip: [.get_dummies()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html) Converts categorical variable into dummy/indicator variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O5gaQhC_aDSI"
   },
   "source": [
    "- Which feature of our dataset is a candidate for performing a one-hot-encoding?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9_lEmKWiAQn0"
   },
   "source": [
    "**Solution**\n",
    "\n",
    "Categorical variables such as Geography and Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "byKT5pdPAQn0"
   },
   "outputs": [],
   "source": [
    "# CODE HERE\n",
    "# Define the categorical variables to be encoded\n",
    "multi_cols = ['Geography']\n",
    "\n",
    "# Performs the one-hot-encoding\n",
    "data = pd.get_dummies(data = data, columns = multi_cols)\n",
    "data.iloc[:,-3:].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GlsejsVDAQn2"
   },
   "source": [
    "[**EX 7**]\n",
    "\n",
    "In order to build up the model and increase its efficiency, scale numerical variables to mean = 0 and std = 1.\n",
    "\n",
    "Tip: [StandardScaler()](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) Standardize features by removing the mean and scaling to unit variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ejCMyvaoaDSI"
   },
   "source": [
    "- Which numerical variables are candidates to performe a standarization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UqLAv571AQn3"
   },
   "source": [
    "**Solution**\n",
    "\n",
    "The numerical variables to be scaled are:\n",
    "\n",
    "CreditScore, Age, Tenure, Balance, NumberOfProducts, EstimatedSalary are candidates. Really we only need CreditScore, Balance, and EstimatedSalary since those are the ones that have the largest numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CG6-F2qCAQn3"
   },
   "outputs": [],
   "source": [
    "# Build up the standard scaler\n",
    "std = StandardScaler()\n",
    "\n",
    "# CODE HERE\n",
    "to_scale_variables = ['CreditScore', 'Balance', 'EstimatedSalary']\n",
    "\n",
    "# Transform the data\n",
    "scaled = std.fit_transform(data[to_scale_variables])\n",
    "scaled = pd.DataFrame(scaled, columns=to_scale_variables)\n",
    "\n",
    "# Join both dataframes\n",
    "data.rename(columns = {column: column + \"_original\" for column in to_scale_variables}, inplace=True)\n",
    "data = data.merge(scaled,left_index=True,right_index=True,how = \"left\")\n",
    "data.drop(columns = [column for column in data.columns if \"_original\" in column], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bm5WBJmbAQn5"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3n68_K_lJmvB"
   },
   "source": [
    "# **2. Model Training: Building a churn prediction model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y3CQ_oEBJmvC"
   },
   "source": [
    "Now we are ready to enter in the training stage of the machine learning models. The common way to procedure is starting with baseline models (i.e. SVM, Decision Trees, Naive Bayes, etc....) and later, try to improve it adjusting hyperparameters of the models or creating more complex models architectures as ensembles. In this case we will only improve the model by adjusting its hyperparameters.\n",
    "\n",
    "There is a very important step when building a model, and it is to choose a right model threshold (usually known as model cutoff value). This threshold modifies the output class probability density function in order to balance the model and fit our requirements. for example, imagine a case in which you have to design a model to detect sick people in order to speed up their medical treatment. Is it better either to classify healthy people as sick people or sick people as healthy ones? Normally, it is better to classify healthy people as sick ones, because they will be removed after the medical tests, than sick people as healthy ones, because they won't be treated and maybe will die ... Therefore, the model cutoff plays a really important role when designing a model. Some libraries already has the cutoff as an input parameter, but normally it is not and by default is assigned to 0.5. In this case, what you have to do is to train your model, predict (using probabilities) and modify the prediction value according to your threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tLLkPABNAQn8"
   },
   "source": [
    "The function *train_model* is a function that trains the model and plots the most interesting, and important, metrics that will help you to rightly define the model hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lcixRttyAQn9"
   },
   "outputs": [],
   "source": [
    "def train_model_and_performance_eval(logit,train_x,test_x,train_y,test_y, cols, cutoff = 0.5, cf = 'coefficients'):\n",
    "    logit.fit(train_x,train_y)\n",
    "\n",
    "    predictions = logit.predict(test_x)\n",
    "    probabilities = logit.predict_proba(test_x)\n",
    "\n",
    "    predictions = np.where(probabilities[:,1] >= cutoff, 1, 0)\n",
    "\n",
    "    # Calculate the coefficients dataframe depending on if they are real coefficients or features\n",
    "    if   cf == \"coefficients\" :\n",
    "        coefficients  = pd.DataFrame(logit.coef_.ravel())\n",
    "    elif cf == \"features\" :\n",
    "        coefficients  = pd.DataFrame(logit.feature_importances_)\n",
    "\n",
    "    # Set the coefficients to be shown\n",
    "    column_df = pd.DataFrame(cols)\n",
    "    coef_sumry = (pd.merge(coefficients,column_df,left_index= True,\n",
    "                              right_index= True, how = \"left\"))\n",
    "    coef_sumry.columns = [\"coefficients\",\"features\"]\n",
    "    coef_sumry = coef_sumry.sort_values(by = \"coefficients\",ascending = False)\n",
    "\n",
    "    print(f'{logit}\\n\\nClassification report:\\n{classification_report(test_y,predictions)}')\n",
    "    print(f'\\nAccuracy Score: {accuracy_score(test_y,predictions)}')\n",
    "\n",
    "    # Plot confusion matrix (see plot_confusion_matrix function)\n",
    "    # cm = plot_confusion_matrix(logit, test_x, test_y, cmap=plt.cm.Reds)\n",
    "    cm = confusion_matrix(test_y, predictions, labels=logit.classes_)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=logit.classes_)\n",
    "    disp.plot()\n",
    "    # disp.show()\n",
    "\n",
    "    fig = disp.ax_.get_figure()\n",
    "    fig.set_figwidth(5)\n",
    "    fig.set_figheight(5)\n",
    "    plt.grid(False)\n",
    "\n",
    "    # Plot the probability density chart\n",
    "    model_and_real_values = pd.DataFrame({\"real_value\": predictions, \"predicted_value\": probabilities[:, 1]})\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    sns.kdeplot(data=model_and_real_values, x=\"predicted_value\", fill=True, hue=\"real_value\", label='Predicted Probability Density')\n",
    "    plt.axvline(x=cutoff)\n",
    "    plt.title('Probability Density Chart')\n",
    "\n",
    "    # Plot feature importance bar\n",
    "    # Prepare Data\n",
    "    coef_sumry.reset_index(inplace=True)\n",
    "    coef_sumry['colors'] = ['red' if x < 0 else 'green' for x in coef_sumry['coefficients']]\n",
    "    coef_sumry\n",
    "\n",
    "    # Draw plot\n",
    "    plt.figure(figsize=(14,10), dpi= 80)\n",
    "    plt.hlines(y=coef_sumry.index, xmin=0, xmax=coef_sumry.coefficients, color=coef_sumry.colors, alpha=0.4, linewidth=5)\n",
    "\n",
    "    # Decorations\n",
    "    plt.gca().set(ylabel='$Features$', xlabel='$Coefficients$')\n",
    "    plt.yticks(coef_sumry.index, coef_sumry.features, fontsize=12)\n",
    "    plt.title('Feature Importance', fontdict={'size':20})\n",
    "    plt.grid(linestyle='--', alpha=0.5)\n",
    "    plt.show()\n",
    "\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JQu8XX7rAQoA",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Getting columns to be used when training\n",
    "cols = [i for i in data.columns if i not in ('RowNumber', 'CustomerId', 'Surname', 'Exited')]\n",
    "\n",
    "# Split train, validation and test data\n",
    "train_val, test = train_test_split(data, test_size = .1 ,random_state = 1)\n",
    "train, val = train_test_split(train_val, test_size = .3 ,random_state = 1)\n",
    "\n",
    "train_x = train[cols]\n",
    "train_y = train[\"Exited\"]\n",
    "val_x = val[cols]\n",
    "val_y = val[\"Exited\"].values.ravel()\n",
    "\n",
    "# Change the hyperparameters in order and try to get the best model\n",
    "logit  = LogisticRegression(C=1.0, max_iter=100, solver='liblinear')\n",
    "\n",
    "# Train the model\n",
    "predictions = train_model_and_performance_eval(logit, train_x, val_x, train_y, val_y, cols, cutoff = 0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_16YZ2NtAQoC"
   },
   "source": [
    "[**EX 8**]  [**Play with the hyperparameters of the functions above**]\n",
    "\n",
    "Calculate the best model and define the process you have followed to achieve it. Some questions to answer are:\n",
    "\n",
    "- Which variables you used and why to train the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear', 'saga'],\n",
    "    'max_iter': [100, 200, 500]\n",
    "}\n",
    "\n",
    "# https://stackoverflow.com/questions/66938102/hide-scikit-learn-convergencewarning-increase-the-number-of-iterations-max-it\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "    \n",
    "    # https://datascience.stackexchange.com/questions/30881/when-is-precision-more-important-over-recall\n",
    "    grid = GridSearchCV(LogisticRegression(), param_grid, cv=5, scoring='recall')\n",
    "    grid.fit(train_x, train_y)\n",
    "    # we should take into account validation set too\n",
    "\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "print(\"Best recall:\", grid.best_score_)\n",
    "\n",
    "best_logit = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KNY27NcV1aIb"
   },
   "outputs": [],
   "source": [
    "# Change the hyperparameters in order and try to get the best model\n",
    "logit  = LogisticRegression(C=10, max_iter=100, solver='liblinear', penalty='l1')\n",
    "\n",
    "# Train the model\n",
    "predictions = train_model_and_performance_eval(logit, train_x, val_x, train_y, val_y, cols, cutoff = 0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y8-remqzaDSQ"
   },
   "source": [
    "#### Solution:\n",
    "\n",
    "Since the dataset is relatively small we can bruteforce it using grid search to find the most optimal parameters. We want to maximize recall in order to minimize false negatives, since in this use case it is more important to not lose a client and thinking that it wasn't going to churn than to not lose it and thinking he was going to leave. With this approach we find that the best model among the tried ones is \n",
    "```\n",
    "LogisticRegression(C=10, max_iter=100, solver='liblinear', penalty='l1')\n",
    "```\n",
    "\n",
    "However it might not be worth it since there are too many false positives, so we might need to take into account other metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RnjiFO7B1aIc"
   },
   "source": [
    "[**EX 9**]\n",
    "\n",
    "- Do you think the density chart is balanced?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r5bGlLUw1aIc"
   },
   "source": [
    "#### Solution:\n",
    "\n",
    "The density chart does not seem to be balanced. Both churn and not churn resemble a poisson distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lgKSnzq-1aIc"
   },
   "source": [
    "[**EX 10**]  **Confusion Matrix**\n",
    "- What do the confusion matrix values mean?\n",
    "- In this case do you think it is worse to have false positives or false negatives?\n",
    "- Do you think the model works well based on the confusion matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oOQ9gQj91aIc"
   },
   "source": [
    "#### Solution:\n",
    "\n",
    "- The confusion matrix values are (clock-wise order): true negatives, false negatives, true positives, false positives.\n",
    "- It is worse to have more false negatives.\n",
    "- This model prioritizes false negatives, however it might be too much. There are too many false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4fuf8AIeaDSQ"
   },
   "source": [
    "**[EX 11]  [Play with the cutoff]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0VzcAsUYaDSQ"
   },
   "source": [
    "- What happens if you move the cutoff to the right?\n",
    "- and to the left?\n",
    "- How many customers are at risk of churn?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SdQSI1rcaDSQ"
   },
   "source": [
    "#### Solution:\n",
    "\n",
    "If we move the cutoff to the right (e.g.: 0.9) we only churn if we are more than 90% sure that they are going to churn. The opposite happens with the left. The number of customers \"at risk\" depends on this parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "su7xKpRIAQoC"
   },
   "source": [
    "[**EX 12**]\n",
    "\n",
    "Predict over the test dataset (using the original dataset values) which customers are possible churners or not.\n",
    "\n",
    "- PossibleChurner: value 0/1\n",
    "- Possiblechurner_Proba: probabilities (0, ..., 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o_DgolI3AQoD"
   },
   "source": [
    "**Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ip2LoVsAQoD"
   },
   "outputs": [],
   "source": [
    "original_test = original_data[original_data.CustomerId.isin(test.CustomerId.values)].sort_values(by = [\"CustomerId\"])\n",
    "test = test.sort_values(by = [\"CustomerId\"])\n",
    "\n",
    "original_test['PossibleChurner_Proba'] = logit.predict_proba(test[cols])[:,1]\n",
    "original_test['PossibleChurner'] = np.where(original_test['PossibleChurner_Proba'] >= 0.6, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XZxbp1_1aDSR"
   },
   "outputs": [],
   "source": [
    "original_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lnk9ERTO1aIe"
   },
   "source": [
    "[**EX 13**]\n",
    "\n",
    "Plot the probability density chart of the new predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UVVyIfWraDSR"
   },
   "outputs": [],
   "source": [
    "# CODE HERE\n",
    "\n",
    "# Plot the probability density chart\n",
    "model_and_real_values = pd.DataFrame({\"real_value\": original_test['PossibleChurner'], \"predicted_value\": original_test['PossibleChurner_Proba']})\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.kdeplot(data=model_and_real_values, x=\"predicted_value\", fill=True, hue=\"real_value\", label='Predicted Probability Density')\n",
    "plt.axvline(x=0.6)\n",
    "plt.title('Probability Density Chart')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iv5cG0rtD9Z6"
   },
   "source": [
    "[**EX 14**]\n",
    "\n",
    "- In the new prediction, how do you think the model is performing?\n",
    "- How could we improve it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ug53u9H1D9Z7"
   },
   "source": [
    "**Solution**\n",
    "\n",
    "I think that the model generalizes quite well. I do not know how would I improve it."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "532.344px",
    "left": "1827.97px",
    "right": "20px",
    "top": "118.988px",
    "width": "502.969px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
