{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f8107905",
      "metadata": {
        "id": "intro-cell"
      },
      "source": [
        "# Lab 5: Loan Approval Prediction (Evaluable)\n",
        "\n",
        "# Lab Goals:\n",
        "In this project, we will develop and apply supervised classification methods to predict loan approval. \n",
        "\n",
        "We will follow the end-to-end **Machine Learning process** for a **financial risk use case**:\n",
        "1.  **Data Understanding and Preparation**: Exploration of the dataset (EDA), feature engineering (missing values, categorical variables management).\n",
        "2.  **Model Training**: Training a **Logistic Regression** model.\n",
        "3.  **Model Evaluation & Business Insights**: Analysis of metrics (confusion matrix, precision, recall, density chart). We will analyze how to adjust the model's cut-off based on business goals and how to improve the model.\n",
        "\n",
        "# Practice Information:\n",
        "\n",
        "**Due date:** By end of October, 24th (23:59h)\n",
        "\n",
        "**Submission procedure:** via Moodle.\n",
        "\n",
        "**Name:** Luca Franceschi\n",
        "\n",
        "**NIA:** 253885"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "687b9558",
      "metadata": {},
      "source": [
        "## Part 0: Setup & Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6aa944fb",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import itertools\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff032dcb",
      "metadata": {},
      "source": [
        "### Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65997822",
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv('loan_dataset.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a7f4b01",
      "metadata": {},
      "source": [
        "**Dataset description:**\n",
        "- Loan_ID: A unique identification number for each loan application.\n",
        "- Gender: The applicant's gender (e.g., Male, Female).\n",
        "- Married: The applicant's marital status (e.g., Yes, No).\n",
        "- Dependents: The number of dependents (e.g., children or other family members) the applicant has (e.g., 0, 1, 2, 3+).\n",
        "- Education: The applicant's highest level of education (e.g., Graduate, Not Graduate).\n",
        "- Self_Employed: Whether the applicant is self-employed (e.g., Yes, No).\n",
        "- ApplicantIncome: The applicant's monthly income.\n",
        "- CoapplicantIncome: The co-applicant's monthly income (if one exists, otherwise 0).\n",
        "- LoanAmount: The total amount of the loan requested (in thousands).\n",
        "- Loan_Amount_Term: The duration of the loan in months (e.g., 360 months = 30 years).\n",
        "- Credit_History: A binary flag indicating if the applicant has a credit history that meets the bank's guidelines (1.0 = Yes, 0.0 = No).\n",
        "- Property_Area: The type of area where the property is located (e.g., Urban, Semiurban, Rural).\n",
        "- Loan_Status: (This is the target variable) The final decision on the loan application (Y = Approved, N = Rejected)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22f738d0",
      "metadata": {},
      "source": [
        "## Part 1: Exploratory Data Analysis (EDA)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "daba6c18",
      "metadata": {},
      "source": [
        "### EX1: Initial Data Inspection\n",
        "\n",
        "Get a first look at the data. Answer the questions:\n",
        "- How many total rows (applicants) and columns (features) are in this dataset?\n",
        "- Look at the Non-Null Count for all columns. Which columns have missing (null) values?\n",
        "- What is the data type (Dtype) of the Credit_History column and the ApplicantIncome column?\n",
        "- Look at the first 5 rows of data. What is our target variable (the column we want to predict), and what are its possible values?\n",
        "- What is the average ApplicantIncome?\n",
        "- Look at the min, max, and mean for the Credit_History column. What does this tell you about the data in this column?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce8f3b01",
      "metadata": {},
      "outputs": [],
      "source": [
        "### YOUR CODE HERE\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a5adbb9",
      "metadata": {},
      "source": [
        "- There are 614 rows and 13 columns.\n",
        "- Gender, Married, Dependents, Self_Employed, LoanAmount, Loan_Amount_Term, Credit_History have null values.\n",
        "- Credit_History is float64 and ApplicantIncome is int64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c99ae443",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bbf8fd4",
      "metadata": {},
      "source": [
        "- The target variable is Loan_Status and its possible values are Y/N (binary outcome)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5df002be",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f'Average applicant monthly income is {np.average(df['ApplicantIncome'].to_numpy()):.2f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "336a11ee",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f55f2e0",
      "metadata": {},
      "source": [
        "- The Credit_Score variable seems normalized to the range [0, 1] and is also very right-skewed. It also seems to be a binary variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### EX2: Analyze the Target Variable\n",
        "\n",
        "Our target variable is `Loan_Status`. \n",
        "1.  Use `seaborn.countplot` to visualize the distribution of `Loan_Status`.\n",
        "2.  Print the value counts the proportion.\n",
        "3.  Is the dataset balanced or imbalanced?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### YOUR CODE HERE\n",
        "sns.countplot(df, x='Loan_Status', stat='proportion')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a180ee9",
      "metadata": {},
      "source": [
        "I would say it is not balanced."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### EX3: Visualize all features vs our target to understand the data\n",
        "\n",
        "Let's understand the data and its relation to our target.\n",
        "\n",
        "In order to do so, apply all we have learnt to to understand the numerical and categorical columns.\n",
        "\n",
        "At the end answer the questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6cfa4c3f",
      "metadata": {},
      "outputs": [],
      "source": [
        "### YOUR CODE HERE\n",
        "df['Loan_Status'] = np.where(df['Loan_Status']=='Y', 1, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sns.heatmap(df.corr(numeric_only=True), annot=True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88535f25",
      "metadata": {},
      "source": [
        "- Based on the correlation output, which single numerical feature has the strongest (highest) correlation with Loan_Status?\n",
        "\n",
        "The single numerical feature that has the strongest correlation with Loan_Status is **Credit_History**\n",
        "\n",
        "- Look at the plot for Credit_History. Does having a Credit_History (value 1.0) seem to have a weak, moderate, or strong association with getting a loan approved?\n",
        "\n",
        "It seems to have a moderate-to-strong positive correlation with getting a loan approved.\n",
        "\n",
        "- In our previous exercise, we saw that Credit_History is missing 50 rows and Gender is missing 13 rows. Which of these missing values is \"worse\" or \"scarier\" for our model?\n",
        "\n",
        "Credit_History missing values are worse than Gender missing ones since the correlation with our target variable is not that important for the Gender variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### EX4: Analyze Numerical Features\n",
        "\n",
        "Let's see how `ApplicantIncome` relates to `Loan_Status`.\n",
        "\n",
        "1.  Create a `seaborn.boxplot` with `x='Loan_Status'` and `y='ApplicantIncome'`.\n",
        "2.  Give an insight about what you see.\n",
        "3.  **Extra:** The plot is skewed by outliers. Create a new, filtered DataFrame and plot the boxplot again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### YOUR CODE HERE\n",
        "sns.boxplot(df, x='Loan_Status', y='ApplicantIncome')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Insight:** "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c231bc2",
      "metadata": {},
      "source": [
        "There does not seem to be a significant correlation between Loan_Status variables regarding to ApplicantIncome, since both boxplots are pretty much the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8052b964",
      "metadata": {},
      "outputs": [],
      "source": [
        "incomes = df['ApplicantIncome'].to_numpy()\n",
        "q1 = np.quantile(incomes, .25); q3 = np.quantile(incomes, .75)\n",
        "iqr = q3-q1\n",
        "\n",
        "df_f = df[(df['ApplicantIncome'] > q1-1.5*iqr) & (df['ApplicantIncome'] < q3+1.5*iqr)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1da87aaf",
      "metadata": {},
      "outputs": [],
      "source": [
        "sns.boxplot(df_f, x='Loan_Status', y='ApplicantIncome')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### EX5: Check for Missing Values\n",
        "\n",
        "- How many missing values do we have in each feature?\n",
        "- How many rows would we end up with if we dropped all rows with at least one missing value?\n",
        "\n",
        "Use `.isnull().sum()` to find all columns with missing values and print the result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### YOUR CODE HERE\n",
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62f25575",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.dropna(how='any').info()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5da45bbb",
      "metadata": {},
      "source": [
        "We would end up with 480 rows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Data Preparation & Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5532d278",
      "metadata": {},
      "source": [
        "### EX6: Handle Missing Values\n",
        "\n",
        "We must clean the data before we can train a model.\n",
        "\n",
        "Inputing values inserts noise to our dataframe since the inputation can be wrong, hence lying to our model.\n",
        "\n",
        "QUESTION: Is it worth in this situation to impute values or would it be better to just drop the rows?\n",
        "\n",
        "ANSWER: **IMO the best approach would be to impute the cells that belong to low-correlation features and drop the ones that belong to a high-correlation one. This way we do not risk to generate a ton of error while keeping values that we might be very important for the later analysis.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9a3317a",
      "metadata": {},
      "source": [
        "### EX7: Understand, complete and execute the following code\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Handle Missing Values (Categorical)\n",
        "\n",
        "It's best to impute missing values. For categorical features, we will use the **mode** (the most frequent value).\n",
        "\n",
        "Steps:\n",
        "1.  Create a list `cat_cols_with_na` with the names of the *categorical* columns that have missing values.\n",
        "2.  Loop through this list and fill the `NA` values in `df` with the mode of each column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43eddb5c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# We create a copy to avoid changing the original df\n",
        "df_processed = df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### FILL COLUMN LIST\n",
        "cat_cols_with_na = ['Gender', 'Married', 'Dependents', 'Self_Employed']\n",
        "\n",
        "for col in cat_cols_with_na:\n",
        "    mode_val = df_processed[col].mode()[0]\n",
        "    df_processed[col] = df_processed[col].fillna(mode_val)\n",
        "    print(f\"Filled NAs in '{col}' with mode: {mode_val}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Handle Missing Values (Numerical)\n",
        "\n",
        "For numerical features, we will use the **median** (which is robust to outliers).\n",
        "\n",
        "1.  Create a list `num_cols_with_na`.\n",
        "2.  Loop through this list and fill the `NA` values in `df_processed` with the **median** of each column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### FILL COLUMN LIST\n",
        "num_cols_with_na = ['LoanAmount', 'Loan_Amount_Term', 'Credit_History']\n",
        "\n",
        "for col in num_cols_with_na:\n",
        "    median_val = df_processed[col].median()\n",
        "    df_processed[col] = df_processed[col].fillna(median_val)\n",
        "    print(f\"Filled NAs in '{col}' with median: {median_val}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### EX8: Understand and execute the following code\n",
        "\n",
        "Let's create a `TotalIncome` feature by combining `ApplicantIncome` and `CoapplicantIncome`.\n",
        "\n",
        "1.  Create `df_processed['TotalIncome']` by adding the two columns.\n",
        "2.  Drop the columns that we don't need for the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_processed['TotalIncome'] = df_processed['ApplicantIncome'] + df_processed['CoapplicantIncome']\n",
        "\n",
        "# Drop non needed columns for the model\n",
        "columns_to_drop = ['ApplicantIncome', 'CoapplicantIncome', 'Loan_ID']\n",
        "df_processed = df_processed.drop(columns_to_drop, axis=1)\n",
        "\n",
        "print(\"Created 'TotalIncome' and dropped original columns.\")\n",
        "df_processed.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### EX9: Convert Target Variable\n",
        "\n",
        "Machine learning models need numbers, not 'Y' and 'N'.\n",
        "\n",
        "1.  Map `Loan_Status` to boolean (1 for 'Y' and 0 for 'N')."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### YOUR CODE HERE\n",
        "# already done before\n",
        "# df_processed['Loan_Status'] = ...\n",
        "# df['Loan_Status'] = np.where(df['Loan_Status']=='Y', 1, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1286772c",
      "metadata": {},
      "source": [
        "## Part 3: Model Preparation (Split & Scale)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7cee3dbb",
      "metadata": {},
      "source": [
        "### Understand and execute the following code\n",
        "\n",
        "1.  Create `X` by dropping `Loan_Status` from `df_final`.\n",
        "2.  Create `y` with only the `Loan_Status` column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a0740c5",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_final = df_processed.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a3ea439",
      "metadata": {},
      "outputs": [],
      "source": [
        "X = df_final.drop('Loan_Status', axis=1)\n",
        "y = df_final['Loan_Status']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f167f5f",
      "metadata": {},
      "source": [
        "Split the Data\n",
        "\n",
        "1.  `train_test_split` to create `X_train`, `X_test`, `y_train`, `y_test`.\n",
        "2.  `test_size=0.2`, `random_state=42`, and **`stratify=y`** (important for imbalanced datasets)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8f02be1",
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "651c498e",
      "metadata": {},
      "source": [
        "### One-Hot Encode Categorical Features\n",
        "\n",
        "Models can't understand text like 'Male', 'Urban', etc. We need to convert them to dummy variables.\n",
        "\n",
        "1.  `Credit_History` is 1.0/0.0, but should be treated as categorical. Convert it to `str` type: `df_processed['Credit_History'] = df_processed['Credit_History'].astype(str)`\n",
        "2.  Get a list of all `object` type columns: `categorical_cols = df_processed.select_dtypes(include=['object']).columns`\n",
        "3.  Use `pd.get_dummies()` on `df_processed`, passing the `categorical_cols` to the `columns` argument, and set `drop_first=True` (to avoid multicollinearity).\n",
        "4.  Store the result in a new DataFrame `df_final`.\n",
        "\n",
        "\n",
        "\n",
        "### Feature Scalling\n",
        "\n",
        "Logistic Regression is sensitive to feature scales. We must scale our data as we did in previous labs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecc954b8",
      "metadata": {},
      "outputs": [],
      "source": [
        "categorical_cols = ['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Property_Area']\n",
        "numeric_cols = ['LoanAmount', 'Loan_Amount_Term', 'Credit_History', 'TotalIncome']\n",
        "\n",
        "\n",
        "# Train SCALER (numeric columns only)\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train[numeric_cols])\n",
        "\n",
        "# Train ENCODER (categoric columns only)\n",
        "encoder = OneHotEncoder(handle_unknown='ignore', drop='first', sparse_output=False)\n",
        "encoder.fit(X_train[categorical_cols])\n",
        "\n",
        "# Transform from the encoder trained\n",
        "X_train_scaled_data = scaler.transform(X_train[numeric_cols])\n",
        "df_train_scaled = pd.DataFrame(X_train_scaled_data, columns=numeric_cols, index=X_train.index)\n",
        "\n",
        "X_train_encoded_data = encoder.transform(X_train[categorical_cols])\n",
        "encoded_col_names = encoder.get_feature_names_out(categorical_cols)\n",
        "df_train_encoded = pd.DataFrame(X_train_encoded_data, columns=encoded_col_names, index=X_train.index)\n",
        "\n",
        "# Concat all features\n",
        "X_train_processed_df = pd.concat([df_train_scaled, df_train_encoded], axis=1)\n",
        "\n",
        "# Store the model columns\n",
        "model_columns_ordered = X_train_processed_df.columns.tolist()\n",
        "\n",
        "print(f\"Número total de columnes del model: {len(model_columns_ordered)}\")\n",
        "print(\"Llista de columnes del model:\", model_columns_ordered)\n",
        "display(X_train_processed_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5c6827a",
      "metadata": {},
      "source": [
        "### IMPORTANT\n",
        "\n",
        "The .fit() and .transform() steps for the scalers and encoders are done separately. This is because we .fit() them on our training data and then store these same fitted objects (e.g., in a pickle file) for our Streamlit app. The app will then use these fitted objects to .transform() new user input.\n",
        "\n",
        "The .fit() step is done only on the train data (X_train) to prevent data leakage.\n",
        "\n",
        "- Data Leakage is when information from your test set (which is supposed to be \"unseen\") leaks into your training process.\n",
        "\n",
        "- If we .fit() the scaler on the entire dataset, it would learn the mean and standard deviation of the test data. If we fit the encoder on the entire dataset, it would learn all possible categories from the test data.\n",
        "\n",
        "- This is \"cheating.\" It gives our model information about the future, leading to falsely optimistic performance scores (like high accuracy) in our notebook.\n",
        "\n",
        "- When the model goes live (in the Streamlit app) and sees truly new data, its performance will be much worse than we expected. By fitting only on X_train, we simulate this real-world scenario and get a true, reliable score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_logistic_regression(X_train, y_train):\n",
        "    \"\"\"\n",
        "    Train a Logistic Regression model on the training data.\n",
        "    We use class_weight='balanced' to handle imbalance.\n",
        "    \"\"\"\n",
        "    # Initialize the Logistic Regression model\n",
        "    model = LogisticRegression(random_state=42, class_weight='balanced')\n",
        "    \n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train)\n",
        "    \n",
        "    return model\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    #print(cm)\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.tight_layout()\n",
        "\n",
        "def density_chart(data, label, class_names, model, cutoff=0.5):\n",
        "    \"\"\"\n",
        "    Plots a density chart of the predicted probabilities for each class.\n",
        "    \"\"\"\n",
        "    # Get the probabilities for each class\n",
        "    probabilities = model.predict_proba(data)\n",
        "    \n",
        "    # Get the probability of the positive class (class 1)\n",
        "    prob_positive = probabilities[:, 1]\n",
        "    \n",
        "    # Create a DataFrame for plotting\n",
        "    df_plot = pd.DataFrame({'Probability': prob_positive, 'Label': label})\n",
        "    \n",
        "    # Plot the density chart\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.kdeplot(data=df_plot, x='Probability', hue='Label', common_norm=False, fill=True, palette='coolwarm', legend=class_names)\n",
        "    \n",
        "    # Customize the plot\n",
        "    plt.title('Density Chart of Predicted Probabilities', fontsize=16)\n",
        "    plt.xlabel('Predicted Probability', fontsize=12)\n",
        "    plt.ylabel('Density', fontsize=12)\n",
        "    plt.axvline(x=cutoff, color='red', linestyle='--', label=f'Cut-off ({cutoff})')\n",
        "    # plt.legend(title='Actual Label', labels=class_names)\n",
        "    plt.show()\n",
        "\n",
        "def evaluate_model_at_cutoff(model, X_test, y_test, cutoff=0.5):\n",
        "    \"\"\"\n",
        "    Evaluates a model at a specific probability cutoff.\n",
        "    \n",
        "    It calculates predictions, prints a classification report,\n",
        "    and calls the 'density_chart' and 'plot_confusion_matrix' functions.\n",
        "    \"\"\"\n",
        "    class_names=['Rejected (0)', 'Approved (1)']\n",
        "    \n",
        "    # 1. Get probability of positive class (class 1)\n",
        "    y_prob = model.predict_proba(X_test)[:, 1]\n",
        "    \n",
        "    # 2. Generate predictions based on the custom cutoff\n",
        "    y_pred = (y_prob >= cutoff).astype(int)\n",
        "    \n",
        "    print(f\"      EVALUATION REPORT AT CUT-OFF = {cutoff}      \")\n",
        "    \n",
        "    # 3. Print the classification report\n",
        "    print(\"\\n--- Classification Report ---\")\n",
        "    print(classification_report(y_test, y_pred, target_names=class_names))\n",
        "    \n",
        "    # 4. Call the density_chart function\n",
        "    # Aquesta funció ja crea la seva pròpia figura\n",
        "    print(\"\\n--- Probability Density Plot ---\")\n",
        "    density_chart(data=X_test, \n",
        "                  label=y_test, \n",
        "                  class_names=class_names, \n",
        "                  model=model, \n",
        "                  cutoff=cutoff)\n",
        "    \n",
        "    # 5. Calculate and plot the confusion matrix\n",
        "    print(f\"\\n--- Confusion Matrix (at Cut-off = {cutoff}) ---\")\n",
        "    \n",
        "    # Calculate the cm\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    \n",
        "    # Create a new figure for the confusion matrix\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    plot_confusion_matrix(cm=cm, \n",
        "                          classes=class_names, \n",
        "                          title=f'Confusion Matrix (Cut-off = {cutoff})')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### EX10: Execute - Train Logistic Regression\n",
        "\n",
        "1.  Call the `train_logistic_regression` function with your scaled training data to create `model_lr`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### your code here\n",
        "model_lr = train_logistic_regression(X_train_processed_df, y_train)\n",
        "print(model_lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e925cd61",
      "metadata": {},
      "source": [
        "### Model evaluation - Predict the test set\n",
        "\n",
        "First, we need to encode and scale the data as we did with the train set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "589cff78",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scale numeric cols\n",
        "X_test_scaled_data = scaler.transform(X_test[numeric_cols])\n",
        "\n",
        "df_test_scaled = pd.DataFrame(X_test_scaled_data, \n",
        "                            columns=numeric_cols, \n",
        "                            index=X_test.index) # Mantenim l'índex per al concat\n",
        "\n",
        "# Encode categorical cols\n",
        "X_test_encoded_data = encoder.transform(X_test[categorical_cols])\n",
        "\n",
        "encoded_col_names = encoder.get_feature_names_out(categorical_cols)\n",
        "df_test_encoded = pd.DataFrame(X_test_encoded_data, \n",
        "                             columns=encoded_col_names, \n",
        "                             index=X_test.index) # Mantenim l'índex per al concat\n",
        "\n",
        "# Concat both dataframes horizontally\n",
        "X_test_processed_df = pd.concat([df_test_scaled, df_test_encoded], axis=1)\n",
        "\n",
        "# Reorder cols (we need the same order as in training)\n",
        "X_test_processed_df = X_test_processed_df[model_columns_ordered]\n",
        "\n",
        "print(\"Dades de X_test processades correctament.\")\n",
        "display(X_test_processed_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### EX11: Evaluate Logistic Regression\n",
        "\n",
        "1.  Call the function evaluate_model_at_cutoff() with the requested params\n",
        "2.  Question: Which are the main steps of this function?\n",
        "\n",
        "Answer:\n",
        "\n",
        "As specified in the function, the steps are:\n",
        "\n",
        "1. Get probability of positive class (class 1, loan is given)\n",
        "2. Generate predictions based on the cutoff (above cutoff, class 1 selected; below, class 0)\n",
        "3. Print report, density charts and confusion matrix of the prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### your code here\n",
        "evaluate_model_at_cutoff(model_lr, X_test_processed_df, y_test, cutoff=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### EX12: Interpret Logistic Regression Coefficients\n",
        "\n",
        "A huge advantage of Logistic Regression is its interpretability.\n",
        "\n",
        "1.  Create a `pd.DataFrame` to store the `model_lr.coef_[0]` and the `X.columns`.\n",
        "2.  Sort the DataFrame by the coefficient value (descending) and display it.\n",
        "3.  Which feature is the most important positive predictor? Which is a strong negative predictor?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "coefficients = pd.DataFrame({\n",
        "    'Feature': model_columns_ordered,\n",
        "    'Coefficient': model_lr.coef_[0]\n",
        "})\n",
        "\n",
        "coefficients = coefficients.sort_values(by='Coefficient', ascending=False)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.barplot(data=coefficients, x='Coefficient', y='Feature', hue='Feature', palette='vlag', legend=False)\n",
        "plt.title('Feature Importance (Coefficients) from Logistic Regression')\n",
        "plt.show()\n",
        "\n",
        "display(coefficients)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer:** \n",
        "\n",
        "The most important positive predictor is the Credit_History, while a strong negative predictor is Education_Not_Graduate (strongest indeed)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### EX13: Business Scenario 1 (Conservative Bank)\n",
        "\n",
        "**Goal:** Be very conservative and **minimize risk**. We want to avoid approving bad loans.\n",
        "\n",
        "1.  Which are the 'bad loans'? TP, FP, TN, FN?\n",
        "2.  What do we want to maximize here? (answer with a metric)\n",
        "3.  What do we have to do with the cutoff to increase the answer 2 metric?\n",
        "4.  Play with the following cell, define the best cutoff (aprox) and analyze: What happened to Recall? What happened to Precision?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66e5daf3",
      "metadata": {},
      "source": [
        "**Answers:**\n",
        "\n",
        "1. Bad loans are False Positives (FP). They are predicted to be able to pay the loan, but they will default.\n",
        "2. We want to maximize the precision since we, as a conservative bank, cannot stand loan defaults. Mathematically speaking,\n",
        "\n",
        "$$\n",
        "\\lim_{FP \\to 0} \\frac{TP}{TP + FP} = 1\n",
        "$$\n",
        "\n",
        "3. We should increase the cutoff, which will make it more difficult for undecided predictions to pass the cutoff.\n",
        "\n",
        "**NOTE:** We need to take into account that the rate at which FP decrease w.r.t. TP is also important for this analysis. In this case, if the threshold is moved from 0.5 to 0.6 the precision actually decreases. This question is quite complex and cannot be answered easily."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### YOUR CODE HERE\n",
        "new_threshold_recall = 0.7\n",
        "\n",
        "evaluate_model_at_cutoff(model_lr, X_test_processed_df, y_test, cutoff=new_threshold_recall)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### EX14: Business Scenario 2 (Aggressive Growth)\n",
        "\n",
        "**Goal:** Aggressive growth. We want to **capture as many good customers as possible**. We are willing to accept a little more risk to avoid missing opportunities.\n",
        "\n",
        "1.  What do we want to maximize here? (answer with a metric and justify your answer)\n",
        "2.  What do we have to do with the cutoff to increase the answer 1 metric?\n",
        "3.  Play with the following cell, define the best cutoff (aprox) and analyze: What happened to Recall? What happened to Precision?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf949ece",
      "metadata": {},
      "source": [
        "**Answers**\n",
        "\n",
        "1. In this scenario we want to maximize the recall, since we can tolerate a higher amount of FP. We want to get people more loans and we do not care that much if some of them default.\n",
        "2. We can decrease the cutoff. This way, undecided predictions will have a higher change of being accepted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### YOUR CODE HERE\n",
        "new_threshold_recall = 0.3\n",
        "\n",
        "evaluate_model_at_cutoff(model_lr, X_test_processed_df, y_test, cutoff=new_threshold_recall)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "505de750",
      "metadata": {},
      "source": [
        "Lowering the threshold has improven the recall significantly."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "806dca65",
      "metadata": {},
      "source": [
        "### EX15: Our current models are relatively straightforward. Briefly describe two or three techniques you would try next to improve the model's performance.\n",
        "\n",
        "Answer: I would look into optimizing some LogisticRegression hyperparameters, try out other tree-based models such as hierarchical clustering methods, and i would also take a more in-depth look at how the cutoff behaves, maybe even trying to optimize it through a grid-search approach.\n",
        "\n",
        "### EX16: Looking at our EDA and model performance, what do you believe is the single biggest factor limiting our model's accuracy?\n",
        "\n",
        "Answer: Probably the lack of more rows of data and/or features."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "177db879",
      "metadata": {},
      "source": [
        "### EX17: Consider two new applicants. Based on your interpretation of the model's coefficients (from Exercise 6.2), which applicant is more likely to be approved, and why?\n",
        "- Applicant A: Has a Credit_History = 1.0, is Married, and lives in a Semiurban area.\n",
        "- Applicant B: Has a Credit_History = 0.0, is Not Graduate, and is Self_Employed\n",
        "\n",
        "Answer:\n",
        "\n",
        "Applicant A would be more likely to be approved since he has all the positively correlated variables in his favor, while Applicant B is pretty much the opposite."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ad07912",
      "metadata": {},
      "source": [
        "## PART 5. Communicating results through a Visual Analytics web tool."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1494a455",
      "metadata": {},
      "source": [
        "We are ready to share our results with our Consultant team and customers. To do it, we will create a Data product using the Streamlit framework. This web app should have a least 2 tabs: one for data exploration and other for salary prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6661962e",
      "metadata": {},
      "source": [
        "Before building this app, you should store the ML model and labencoders in a **pickle** format as follows. This pickle file will be the one you need to call in your web app."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e11800b",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6061eba",
      "metadata": {},
      "outputs": [],
      "source": [
        "data = {\n",
        "    \"model\": model_lr,\n",
        "    \"encoder\": encoder,\n",
        "    \"scaler\": scaler,\n",
        "    \"numeric_cols\": numeric_cols,\n",
        "    \"categorical_cols\": categorical_cols,\n",
        "    \"model_columns_ordered\": model_columns_ordered,\n",
        "}\n",
        "with open('loan_app_artifacts.pkl', 'wb') as file:\n",
        "    pickle.dump(data, file)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c122bb1a",
      "metadata": {},
      "source": [
        "You can verify the pickle file works as follows: first, open the file storing into 3 variables the regressor, the labencoder for country and the labencoder for education."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3dcb301b",
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('loan_app_artifacts.pkl', 'rb') as file:\n",
        "    data = pickle.load(file)\n",
        "\n",
        "model_loaded = data[\"model\"]\n",
        "encoder_loaded = data[\"encoder\"]\n",
        "scaler_loaded = data[\"scaler\"]\n",
        "numeric_cols_loaded = data[\"numeric_cols\"]\n",
        "categorical_cols_loaded = data[\"categorical_cols\"]\n",
        "model_columns_ordered_loaded = data[\"model_columns_ordered\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a4d5039",
      "metadata": {},
      "source": [
        "You can also make a prediction with a sample as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94c8c3bc",
      "metadata": {},
      "outputs": [],
      "source": [
        "raw_columns = ['LoanAmount', 'Loan_Amount_Term', 'TotalIncome', \n",
        "               'Gender', 'Married', 'Dependents', \n",
        "               'Education', 'Self_Employed', 'Credit_History', 'Property_Area']\n",
        "\n",
        "sample_data = [[\n",
        "    150.0,       # LoanAmount\n",
        "    360.0,       # Loan_Amount_Term\n",
        "    8000.0,      # TotalIncome\n",
        "    'Male',      # Gender\n",
        "    'Yes',       # Married\n",
        "    '0',         # Dependents (recorda que era un 'object', per això va com a string)\n",
        "    'Graduate',  # Education\n",
        "    'No',        # Self_Employed\n",
        "    1.0,         # Credit_History (float, 1.0 o 0.0)\n",
        "    'Semiurban'  # Property_Area\n",
        "]]\n",
        "\n",
        "sample_df = pd.DataFrame(sample_data, columns=raw_columns)\n",
        "display(sample_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2daeb785",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Normalize as in training\n",
        "\n",
        "# Scale numeric cols\n",
        "sample_df_scaled_data = scaler.transform(sample_df[numeric_cols_loaded])\n",
        "\n",
        "df_sample_scaled = pd.DataFrame(sample_df_scaled_data, \n",
        "                            columns=numeric_cols_loaded, \n",
        "                            index=sample_df.index) # Mantenim l'índex per al concat\n",
        "\n",
        "# Encode categorical cols\n",
        "sample_df_encoded_data = encoder.transform(sample_df[categorical_cols_loaded])\n",
        "\n",
        "encoded_col_names = encoder.get_feature_names_out(categorical_cols_loaded)\n",
        "df_sample_encoded = pd.DataFrame(sample_df_encoded_data, \n",
        "                             columns=encoded_col_names, \n",
        "                             index=sample_df.index) # Mantenim l'índex per al concat\n",
        "\n",
        "# Concat both dataframes horizontally\n",
        "sample_df_processed_df = pd.concat([df_sample_scaled, df_sample_encoded], axis=1)\n",
        "\n",
        "# Reorder cols (we need the same order as in training)\n",
        "sample_df_processed_df = sample_df_processed_df[model_columns_ordered]\n",
        "\n",
        "print(\"Dades de sample_df processades correctament.\")\n",
        "display(sample_df_processed_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46eb435e",
      "metadata": {},
      "outputs": [],
      "source": [
        "MY_CUTOFF = 0.5\n",
        "\n",
        "# Predict with our cutoff\n",
        "(model_loaded.predict_proba(sample_df_processed_df)[:, 1] >= MY_CUTOFF).astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "989c0ace",
      "metadata": {},
      "source": [
        "Now, you are ready to go to next step: building the Visual Analytics tool based on Streamlit. Recall, your app should have a main .py file (for instance, **my_app.py**) and a folder called **pages** with all .py files you want to show in your app."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
